Beautiful Soup将复杂的HTML文档转化盛一个复杂的树形结构(即有缩进的形式)，每个节点都是python对象，

soup = BeautifulSoup(html_str, 'lxml', from_encoding='UTF-8')   # 参考BeautifulSoup.py

---->>> 所有对象可以归纳为4种：1.Tag 2.NavigableString 3.BeautifulSoup 4.Comment
1.Tag
标记，如html标记语言中<title>The Dormouse's story</title>的title就是一种Tag
抽取title: soup.title     -->     <title>The Dormouse's story</title>
抽取a: soup.a
抽取p: soup.p

Tag中有两个重要的树形name和attributes 每个Tag都有自己的名字，使用.name获取
print(soup.name)    -->     [document]
print(soup.title.name)  -->     title

可以通过给soup.{Tag}.name赋值来改变Tag的名字，例如
soup.title.name = 'mytitle'
print(soup.title)   --> None
print(soup.mytitle) --> <mytitle>The Dormouse's story</mytitle>

Tag中的属性获取方式与字典相同, <p class="title"><b>The Dormouse's story</b></p>又一个class属性
soup.p['class'] == soup.p.get('class')  --> ['title']
通过.attrs可以获取全部属性信息
print(soup.p.attrs) --> {'class': ['sister']}
仍然可以通过给soup.{Tag}[attr]赋值来改变Tag标记中attr属性的值
soup.p['class'] = 'myClass'

2.NavigableString
获取对应标记中的内容<Tag>内容</Tag>
print(soup.p.string)

3.BeautifulSoup
表示一个文档中的全部内容

4.Comment
表示注释    <a href='http://example.com/elsie' class='sister' id='link1'><!-- Elsie --></a>
print(soup.a.string)会发现只有 Elsie 而没有了注释符号，为了避免数据混乱可以预先判断一下
import bs4
if type(soup.a.string) == bs4.element.Comment:
    print('<!--' + soup.a.string + '-->')


---->>>      遍历文档树
1.子节点(直接子节点).contents .children .descendants
soup.head.contents              -->     返回head标记下的全部直接子节点组成的list
soup.head.contents[0].string    -->     获取head下第一个子节点的内容
soup.children                   -->     作为循环的目标，是一个生成器 for i in soup.children: print(i)
soup.head.descendants           -->     获取head下的所有后代节点, for i in soup.head.descendants

2.获取子节点的内容.string .strings .stripped_strings
.string     -->     当某tag后代中只有一个内容时.string会返回该唯一内容，若后代中有多个内容则返回None
.strings    -->     用于循环遍历多个内容for string in soup.strings: print(repr(string))   repr返回一个string类型
.stripped_strings   -->     与.strings作用相同，额外的删除前后缀的空白字符

3.父节点.parent .parents
.parent     -->     返回当前节点的父节点，soup.title.parent == <head><title>内容</title></head>
.parents    -->     用于递归遍历前辈节点，从内到外(从近到远)输出 for parent in soup.a.parents: if parent not None: print(parent.name)

4.兄弟节点  (字符串也是一个节点，甚至一个换行符等标点符号都是一个节点)
.next_sibling       -->     当前节点的后一个兄弟节点
.previous_sibling   -->     当前节点的前一个兄弟节点
.next_siblings      -->     用于循环遍历之后的所有兄弟节点
.previous_siblings  -->     用于循环遍历之前的所有兄弟节点
.next_element       -->     不分层次访问下一个节点，可以是儿子可以是兄弟
.previous_element   -->     部分层次访问上一个节点，可以是兄弟可以是父亲
.next_elements      -->     迭代器
.previous_elements  -->     迭代器


---->>>         搜索文档树***
find_all()方法用于搜索当前节点的所有子节点
函数原型: find_all(name, attrs, recursive, text, **kwargs)

1.name参数，可以查找所有名字为name的标记，其值可以为字符串，正则表达式，列表，True和方法
print(soup.find_all('b'))       -->     找到所有的b标记，即加粗的标记，返回list
soup.find_all(re.compile('^b')) -->     找到所有以b开头的标记，包括但不限于<b>、<body>，返回list
soup.find_all(["a", "b"])       -->     传入列表将匹配列表中任意值，即找到<a>和<b>
soup.find_all(True)             -->     将匹配所有标记
def hasClass_Id(tag):
    return tag.has_attr('class') and tag.has_attr('id')
soup.find_all(hasClass_Id)      -->     设置一个过滤函数，只能有一个参数tag，选取包含class且包含id两个属性的标记

2.kwargs参数，可以指定属性的值来进行查找，用","隔开多个条件
soup.find_all(id='link2')       -->     Beautiful Soup会搜索所有标记的id属性，若id='link2'则加入返回的list中
soup.find_all(href=re.compile("elsie")  -->     匹配超链接中有"elsie"的标记
soup.find_all(id=True)          -->     找到包含id属性的标记，无论id的值是什么
soup.find_all(class_=True)      -->     由于class是python关键字，要找包含class属性的标记需要在class后加一个下划线_
data_soup = BeautifulSoup('<div data-foo="value>foo!</div>')
soup.find_all(attrs={"data-foo":"value"})   -->     H5中data-*参数不符合python标准，要指定attrs参数来实现

3.text参数，根据内容来查找，返回内容组成的list
soup.find_all(text=re.compile('Dormouse'))  -->     ["The Dormouse's story", "The Dormouse's story"]
soup.find_all(text=['Tillie', ' Elsie ', ' Lacie '])    --> [' Elsie ', ' Lacie ', 'Tillie']
soup.find_all("a", text=' Elsie ')      -->     [<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>]

4.limit参数类似SQL中的limit限制查找的数量
soup.find_all("a", limit=2)     -->     只会输出前2个<a>标记

5.recursive参数
调用tag的find_all()的方法时会查找所有的后代节点，如果只想要直接的子节点，需要指定recursive=False


---->>>     CSS选择器
class前加"."，id前加"#"，使用方法soup.select()方法，返回list
1.通过标记名称进行查找
soup.select('title')            -->     直接查找title标记
soup.select('html head title')  -->     逐层查找title，查找html下head下的title
soup.select('head > title')     -->     查找head的子节点title
soup.select('p > #link2')       -->     查找p的子节点中有属性id且id='link2'的节点
soup.select('#link1 ~ .sister') -->     查找id='link1'之后所有class='sister'兄弟节点
soup.select('#link1 + .sister') -->     查找id='link1'之后的一个class='sister'兄弟的节点

2.通过CSS的类名查找
soup.select('.sister')          -->     查找所有class='sister'的节点

3.通过tag的id查找
soup.select('#link1')           -->     查找所有id='link1'的节点
soup.select('a#link1')          -->     查找所有id='link1'的<a>节点

4.通过是否存在某个属性查找
soup.select('a[href]')          -->     查找所有有href属性的<a>节点

5.通过属性值查找
soup.select("a[href='http://example.com/elsie']")   -->     查找指定地址的<a>节点
soup.select("a[href^='http://example.com/']")       -->     查找以http://example.com/开头的地址<a>节点
soup.select("a[href$='tillie']")                    -->     查找以tillie结尾的地址<a>节点
soup.select("a[href*='.com/el']")                   -->     查找地址中出现过.com/el的<a>节点
